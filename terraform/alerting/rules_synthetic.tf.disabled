# Synthetic Monitoring Alerts for K6 Tests

locals {
  # Define all monitored services
  synthetic_services = [
    "jellyfin",
    "jellyseerr", 
    "lidarr",
    "overseerr",
    "plex-external",
    "plex-internal",
    "prowlarr",
    "radarr",
    "readarr",
    "sabnzbd",
    "sonarr",
    "tautulli"
  ]

  # Critical services that should alert immediately
  critical_services = ["plex-external", "overseerr", "jellyseerr"]
  
  # Services with different thresholds
  service_thresholds = {
    default = {
      availability = 0.95  # 95% success rate
      response_time_p95 = 3000  # 3 seconds
      response_time_p99 = 5000  # 5 seconds
    }
    plex-internal = {
      availability = 0.98  # Higher threshold for internal
      response_time_p95 = 2000  # 2 seconds  
      response_time_p99 = 3000  # 3 seconds
    }
  }
}

resource "grafana_rule_group" "synthetic_monitoring" {
  name             = "synthetic-monitoring"
  folder_uid       = grafana_folder.monitoring.uid
  interval_seconds = 60

  # Service Down Alert - Check failure rate
  rule {
    name        = "SyntheticServiceDown"
    annotations = {
      summary     = "Service {{ $labels.service }} is failing tests"
      description = "{{ $labels.service }} has only {{ $values.A.Value | printf \"%.0f\" }} test data points in last 15 minutes (expected > 10)"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/SyntheticServiceDown"
    }
    labels = {
      severity = "warning"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "10m"
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 900  # 15 minutes to ensure we catch tests that run every 10m
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        # Get success rate for services that have metrics, or 0 for missing ones
        expr = "(avg by (service) (avg_over_time(k6_checks_rate{service=~\"${join("|", local.synthetic_services)}\"}[15m])) * 100) or on() vector(0)"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        # Check if metrics are absent (service down) OR success rate < 95%
        expr = "(absent(k6_checks_rate{service=~\"${join("|", local.synthetic_services)}\"}) == 1) or (avg by (service) (avg_over_time(k6_checks_rate{service=~\"${join("|", local.synthetic_services)}\"}[15m])) < 0.95)"
        refId = "B"
      })
    }
  }

  # Critical Service Down Alert - Higher severity for critical services
  rule {
    name        = "CriticalServiceDown"
    annotations = {
      summary     = "Critical service {{ $labels.service }} is down"
      description = "{{ $labels.service }} has only {{ $values.A.Value | printf \"%.0f\" }} test data points in last 15 minutes (expected > 10)"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/CriticalServiceDown"
    }
    labels = {
      severity = "critical"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "5m"  # Alert faster for critical services
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        # Check if critical services have sent metrics recently
        expr = "count by (service) (count_over_time(k6_http_req_duration_seconds{service=~\"${join("|", local.critical_services)}\"}[15m]))"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        # Alert if critical services have less than 10 metric points in 15 minutes
        expr = "count by (service) (count_over_time(k6_http_req_duration_seconds{service=~\"${join("|", local.critical_services)}\"}[15m])) < 10"
        refId = "B"
      })
    }
  }

  # High Response Time Alert
  rule {
    name        = "ServiceSlowResponse"
    annotations = {
      summary     = "Service {{ $labels.service }} has slow response times"
      description = "{{ $labels.service }} P95 response time is {{ $values.A.Value | printf \"%.0f\" }}ms (threshold > 3000ms)"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/ServiceSlowResponse"
    }
    labels = {
      severity = "warning"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "15m"
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "histogram_quantile(0.95, sum by (service, le) (rate(k6_http_req_duration_seconds_bucket{service=~\"${join("|", local.synthetic_services)}\"}[15m]))) * 1000"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = "-100"
      model          = jsonencode({
        expression = "$A > 3000"
        type = "math"
        refId = "B"
      })
    }
  }

  # HTTP Request Failures Alert
  rule {
    name        = "ServiceHTTPFailures"
    annotations = {
      summary     = "Service {{ $labels.service }} has HTTP failures"
      description = "{{ $labels.service }} has {{ $values.A.Value | printf \"%.1f\" }}% failed HTTP requests (4xx/5xx)"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/ServiceHTTPFailures"
    }
    labels = {
      severity = "warning"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "10m"
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "(avg by (service) (avg_over_time(k6_http_req_failed_rate{service=~\"${join("|", local.synthetic_services)}\"}[15m]))) * 100"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 900
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "(avg by (service) (avg_over_time(k6_http_req_failed_rate{service=~\"${join("|", local.synthetic_services)}\"}[15m]))) > 0.10"
        refId = "B"
      })
    }
  }

  # No Test Data Alert - Detect when tests stop running
  rule {
    name        = "SyntheticTestsMissing"
    annotations = {
      summary     = "Synthetic tests are not running"
      description = "Only {{ $values.A.Value | printf \"%.0f\" }} test iterations in the last 20 minutes (expected > 60)"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/SyntheticTestsMissing"
    }
    labels = {
      severity = "warning"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "5m"
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 1200  # 20 minutes
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "sum(increase(k6_iterations_total[20m])) or vector(0)"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 1200
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "(sum(increase(k6_iterations_total[20m])) or vector(0)) < 60"
        refId = "B"
      })
    }
  }

  # Test Pod Stuck Alert - Detects TestRuns stuck in "started" state for >5 minutes
  rule {
    name        = "SyntheticTestStuck"
    annotations = {
      summary     = "K6 tests are stuck in started state"
      description = "{{ $values.A.Value }} TestRuns have been in 'started' state for more than 5 minutes"
      runbook_url = "https://github.com/seobrien/home-ops/wiki/Alerts/SyntheticTestStuck"
    }
    labels = {
      severity = "warning"
      type     = "synthetic"
      depends_on_prometheus = "true"
    }
    for      = "2m"
    condition = "B"
    no_data_state = "OK"

    data {
      ref_id = "A"
      
      relative_time_range {
        from = 600
        to   = 0
      }
      
      datasource_uid = local.prometheus_pdc_uid
      model          = jsonencode({
        expr = "count((time() - k6_testrun_created_timestamp{stage=\"started\"}) > 300)"
        refId = "A"
      })
    }

    data {
      ref_id = "B"
      
      relative_time_range {
        from = 600
        to   = 0
      }
      
      datasource_uid = "-100"
      model          = jsonencode({
        expression = "$A > 0"  # Any TestRun stuck for >5 minutes is a problem
        type = "math"
        refId = "B"
      })
    }
  }
}