---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

# This taskfile is used to manage certain VolSync tasks for a given application, limitations are described below.
#   1. Fluxtomization, HelmRelease, PVC, ReplicationSource all have the same name (e.g. plex)
#   2. ReplicationSource and ReplicationDestination are a Restic repository
#   3. Applications are deployed as either a Kubernetes Deployment or StatefulSet
#   4. Each application only has one PVC that is being replicated

x-vars: &vars
  APP: '{{.APP}}'
  CLAIM: '{{.CLAIM}}'
  CONTROLLER: '{{.CONTROLLER}}'
  JOB: '{{.JOB}}'
  NS: '{{.NS}}'
  PGID: '{{.PGID}}'
  PREVIOUS: '{{.PREVIOUS}}'
  PUID: '{{.PUID}}'

vars:
  VOLSYNC_RESOURCES_DIR: '{{.ROOT_DIR}}/.taskfiles/volsync/resources'

tasks:

  state-*:
    desc: Suspend or Resume Volsync
    summary: |
      CLUSTER: Cluster to run command against (required)
      STATE: resume or suspend (required)
    cmds:
      - flux {{.STATE}} kustomization volsync
      - flux --namespace {{.NS}} {{.STATE}} helmrelease volsync
      - kubectl --namespace {{.NS}} scale deployment volsync --replicas {{if eq "suspend" .STATE}}0{{else}}1{{end}}
    vars:
      NS: '{{.NS | default "volsync-system"}}'
      STATE: '{{index .MATCH 0}}'
    env: *vars
    requires:
      vars: ['CLUSTER']

  list:
    desc: List snapshots for an application
    summary: |
      NS: Namespace the PVC is in (default: default)
      APP: Application to list snapshots for (required)
    cmds:
      - envsubst < {{.VOLSYNC_RESOURCES_DIR}}/list.tmpl.yaml | kubectl apply -f -
      - bash {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh {{.JOB}} {{.NS}}
      - kubectl --namespace {{.NS}} wait job/{{.JOB}} --for condition=complete --timeout=1m
      - kubectl --namespace {{.NS}} logs job/{{.JOB}} --container main
      - kubectl --namespace {{.NS}} delete job {{.JOB}}
    vars:
      NS: '{{.NS | default "default"}}'
      JOB: volsync-list-{{.APP}}
    env: *vars
    requires:
      vars: ['APP']
    preconditions:
      - which envsubst
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/list.tmpl.yaml
    silent: true

  unlock:
    desc: Unlock a Restic repository for an application
    summary: |
      NS: Namespace the PVC is in (default: default)
      APP: Application to unlock (required)
    cmds:
      - echo "Cleaning up any stuck volsync jobs for {{.APP}}..."
      - kubectl --namespace {{.NS}} delete job --field-selector status.successful==0 -l app.kubernetes.io/name={{.APP}} --ignore-not-found=true
      - kubectl --namespace {{.NS}} delete job volsync-src-{{.APP}}-r2 --ignore-not-found=true
      - kubectl --namespace {{.NS}} delete job {{.JOB}} --ignore-not-found=true
      - echo "Creating unlock job..."
      - envsubst < {{.VOLSYNC_RESOURCES_DIR}}/unlock.tmpl.yaml | kubectl apply -f -
      - bash {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh {{.JOB}} {{.NS}}
      - kubectl --namespace {{.NS}} wait job/{{.JOB}} --for condition=complete --timeout=2m
      - kubectl --namespace {{.NS}} logs job/{{.JOB}} --container r2
      - kubectl --namespace {{.NS}} delete job {{.JOB}}
      - echo "✓ Repository unlocked for {{.APP}}"
    vars:
      NS: '{{.NS | default "default"}}'
      JOB: volsync-unlock-{{.APP}}
    env: *vars
    requires:
      vars: ['APP']
    preconditions:
      - which envsubst
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/unlock.tmpl.yaml
    silent: true

  # To run backup jobs in parallel for all replicationsources:
  #   - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:snapshot APP=$0 NS=$1'
  snapshot:
    desc: Snapshot a PVC for an application
    summary: |
      NS: Namespace the PVC is in (default: default)
      APP: Application to snapshot (required)
    cmds:
      - kubectl --namespace {{.NS}} patch replicationsources {{.APP}}-r2 --type merge -p '{"spec":{"trigger":{"manual":"{{.NOW}}"}}}'
      - bash {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh {{.JOB}} {{.NS}}
      - kubectl --namespace {{.NS}} wait job/{{.JOB}} --for condition=complete --timeout=120m
    vars:
      NOW: '{{now | date "150405"}}'
      NS: '{{.NS | default "default"}}'
      JOB: volsync-src-{{.APP}}-r2
      CONTROLLER:
        sh: '{{.VOLSYNC_RESOURCES_DIR}}/which-controller.sh {{.APP}} {{.NS}}'
    env: *vars
    requires:
      vars: ['APP']
    preconditions:
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/which-controller.sh
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh
      - kubectl --namespace {{.NS}} get replicationsources {{.APP}}-r2

  # To run restore jobs in parallel for all replicationdestinations:
  #    - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:restore APP=$0 NS=$1'
  restore:
    desc: Restore a PVC for an application
    summary: |
      NS: Namespace the PVC is in (default: default)
      APP: Application to restore (required)
      PREVIOUS: Previous number of snapshots to restore (default: 2)
    cmds:
      - { task: .check-backup, vars: *vars }
      - { task: .suspend, vars: *vars }
      - { task: .wipe, vars: *vars }
      - { task: .restore, vars: *vars }
      - { task: .resume, vars: *vars }
    vars:
      NS: '{{.NS | default "default"}}'
      PREVIOUS: '{{.PREVIOUS | default 2}}'
      CONTROLLER:
        sh: '{{.VOLSYNC_RESOURCES_DIR}}/which-controller.sh {{.APP}} {{.NS}}'
      CLAIM:
        sh: kubectl --namespace {{.NS}} get replicationsources/{{.APP}}-r2 --output=jsonpath="{.spec.sourcePVC}"
      PUID:
        sh: kubectl --namespace {{.NS}} get replicationsources/{{.APP}}-r2 --output=jsonpath="{.spec.restic.moverSecurityContext.runAsUser}"
      PGID:
        sh: kubectl --namespace {{.NS}} get replicationsources/{{.APP}}-r2 --output=jsonpath="{.spec.restic.moverSecurityContext.runAsGroup}"
    env: *vars
    requires:
      vars: ['APP']
    preconditions:
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/which-controller.sh

  nuke:
    desc: Nuke all volsync custom resources
    summary: Kill em all. Let Buddah sort em out
    cmds:
      - for: { var: CONTENTS }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          echo "Processing {{.ITEM}}"
          kubectl patch volumesnapshotcontents {{ $items._0 }} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete volumesnapshotcontents {{ $items._0 }} --grace-period=0 --force=true --wait=false --ignore-not-found=true
          echo "done!"
      - for: { var: SNAPS }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl patch volumesnapshot -n {{ $items._0 }} {{ $items._1 }} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete volumesnapshot -n {{ $items._0 }} {{ $items._1 }} --grace-period=0 --force=true --wait=false --ignore-not-found=true
      - for: { var: SRC }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl patch replicationsources -n {{ $items._1 }} {{ $items._0 }} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete replicationsources -n {{ $items._1 }} {{ $items._0 }} --grace-period=0 --force=true --wait=false --ignore-not-found=true
      - for: { var: DEST }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl patch replicationdestinations -n {{ $items._1 }} {{ $items._0 }} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete replicationdestinations -n {{ $items._1 }} {{ $items._0 }} --grace-period=0 --force=true --wait=false --ignore-not-found=true
      - for: { var: PVS }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl patch persistentvolumes -n {{ $items._1 }} {{ $items._0 }} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete persistentvolumes -n {{ $items._1 }} {{ $items._0 }} --grace-period=0 --force=true --wait=false --ignore-not-found=true
      - for: { var: PVCS }
        cmd: |
          kubectl patch persistentvolumeclaims {{.ITEM}} -p '{"metadata":{"finalizers": []}}' --type=merge
          kubectl delete persistentvolumeclaims {{.ITEM}} --grace-period=0 --force=true --wait=false --ignore-not-found=true
    vars:
      SNAPS:
        sh: kubectl get volumesnapshot --all-namespaces --no-headers | awk '{print $1 "/" $2}'
      SRC:
        sh: kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2 "/" $1}'
      DEST:
        sh: kubectl get replicationdestinations --all-namespaces --no-headers | awk '{print $2 "/" $1}'
      CONTENTS:
        sh: kubectl get volumesnapshotcontents --all-namespaces --no-headers | awk '{print $1}'
      PVS:
        sh: kubectl get persistentvolumes --all-namespaces --no-headers | grep dst-dest | awk '{print $1 "/default"}'
      PVCS:
        sh: kubectl get persistentvolumeclaims --all-namespaces --no-headers | grep dst-dest | awk '{print $2}'

  cleanup:
    desc: Delete volume populator PVCs in all namespaces
    cmds:
      - for: { var: DEST }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl delete pvc --namespace {{ $items._0 }} {{ $items._1 }}
      - for: { var: CACHE }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl delete pvc --namespace {{ $items._0 }} {{ $items._1 }}
      - for: { var: SNAPS }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl delete volumesnapshot --namespace {{ $items._0 }} {{ $items._1 }}
    vars:
      DEST:
        sh: kubectl get pvc --all-namespaces --no-headers | grep "dst-dest" | awk '{print $1 "/" $2}'
      CACHE:
        sh: kubectl get pvc --all-namespaces --no-headers | grep "dst-cache" | awk '{print $1 "/" $2}'
      SNAPS:
        sh: kubectl get volumesnapshot --all-namespaces --no-headers | grep "dst-dest" | awk '{print $1 "/" $2}'
    env: *vars

  # Check if backup exists before restore
  .check-backup:
    internal: true
    cmds:
      - |
        echo "Checking backup status for {{.APP}}..."
        LAST_STATUS=$(kubectl --namespace {{.NS}} get replicationsource {{.APP}}-r2 -o jsonpath='{.status.latestMoverStatus.logs}' 2>/dev/null)
        if echo "$LAST_STATUS" | grep -q "Directory is empty"; then
          echo "❌ ERROR: No backup exists for {{.APP}}"
          echo "   The directory was empty during the last backup attempt."
          echo "   This usually means the application was using emptyDir instead of a PVC."
          echo ""
          echo "   To fix this:"
          echo "   1. Ensure the application is using existingClaim instead of emptyDir"
          echo "   2. Manually restore data if you have it elsewhere"
          echo "   3. Run 'task volsync:snapshot APP={{.APP}}' after fixing to create a backup"
          exit 1
        fi
        
        LAST_SYNC=$(kubectl --namespace {{.NS}} get replicationsource {{.APP}}-r2 -o jsonpath='{.status.lastSyncTime}' 2>/dev/null)
        if [ -z "$LAST_SYNC" ]; then
          echo "⚠️  WARNING: No successful backups found for {{.APP}}"
          echo "   Continue anyway? (y/N)"
          read -r response
          if [[ ! "$response" =~ ^[Yy]$ ]]; then
            echo "Restore cancelled."
            exit 1
          fi
        else
          echo "✓ Last successful backup: $LAST_SYNC"
        fi
    env: *vars

  # Suspend the Flux ks and hr
  .suspend:
    internal: true
    cmds:
      - flux --namespace flux-system suspend kustomization {{.APP}}
      - flux --namespace {{.NS}} suspend helmrelease {{.APP}}
      - kubectl --namespace {{.NS}} scale {{.CONTROLLER}} --replicas 0
      - kubectl --namespace {{.NS}} wait pod --for delete --selector="app.kubernetes.io/name={{.APP}}" --timeout=2m
    env: *vars

  # Wipe the PVC of all data
  .wipe:
    internal: true
    cmds:
      - envsubst < {{.VOLSYNC_RESOURCES_DIR}}/wipe.tmpl.yaml | kubectl apply -f -
      - bash {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh {{.JOB}} {{.NS}}
      - kubectl --namespace {{.NS}} wait job/{{.JOB}} --for condition=complete --timeout=120m
      - kubectl --namespace {{.NS}} logs job/{{.JOB}} --container main
      - kubectl --namespace {{.NS}} delete job {{.JOB}}
    vars:
      JOB: volsync-dst-wipe-{{.APP}}  # Created exactly as specified
    env: *vars
    preconditions:
      - which envsubst
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wipe.tmpl.yaml
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh

  # Create VolSync replicationdestination CR to restore data
  .restore:
    internal: true
    cmds:
      - envsubst < {{.VOLSYNC_RESOURCES_DIR}}/replicationdestination.tmpl.yaml | kubectl apply -f -
      - bash {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh restore-{{.APP}} {{.NS}}
      - kubectl --namespace {{.NS}} wait replicationdestination/restore-{{.APP}} --for=condition=synchronizing=false --timeout=120m
      - kubectl --namespace {{.NS}} delete replicationdestination restore-{{.APP}}
    vars:
      JOB: restore-{{.APP}}
    env: *vars
    preconditions:
      - which envsubst
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/replicationdestination.tmpl.yaml
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/wait-for-job.sh

  # Resume Flux ks and hr
  .resume:
    internal: true
    cmds:
      - flux --namespace {{.NS}} resume helmrelease {{.APP}}
      - flux --namespace flux-system resume kustomization {{.APP}}
      - kubectl --namespace {{.NS}} scale {{.CONTROLLER}} --replicas 1
    env: *vars
